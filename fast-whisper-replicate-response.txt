(venv) rgann@ubuntuserver:~/video-processing-service$ python3 debug-replicate.py "https://storage.googleapis.com/rag-widget-audio-testing/audio-chunk-000.mp3" --model-type fast
Testing Replicate API with:
- Audio URL: https://storage.googleapis.com/rag-widget-audio-testing/audio-chunk-000.mp3
- Model ID: vaibhavs10/incredibly-fast-whisper
- Model version: 3ab86df6c8f54c11309d4d1f930ac292bad43ace52d10c80d87eb258b3c9f79c
- Model type: Fast Whisper
- Language: english
- API token: ********bno4

Calling Replicate API...

API call successful!
Output type: <class 'dict'>
Response keys: ['chunks', 'text']
- chunks: [{'text': ' Hey, quick question.', 'timestamp': [0.5, 1.84]}, {'text': ' Did you know that you can run the latest large language models locally on your laptop?', 'timestamp': [1.84, 6.14]}, {'text': " This means you don't have any dependencies on cloud services", 'timestamp': [6.46, 9.64]}, {'text': ' and you get full data privacy while using optimized models to chat,', 'timestamp': [9.84, 13.72]}, {'text': ' uses code assistance and integrate AI into your applications with RAG', 'timestamp': [13.98, 18.02]}, {'text': ' or even agentic behavior.', 'timestamp': [18.22, 20.28]}, {'text': " So today we're taking a look at Ohlama.", 'timestamp': [20.28, 22.48]}, {'text': " It's a developer tool that has been quickly growing in popularity,", 'timestamp': [22.52, 25.5]}, {'text': " and we're going to show you how you can start using it on your machine right now.", 'timestamp': [25.78, 29.32]}, {'text': ' But real quick, before we start installing things,', 'timestamp': [29.54, 32.34]}, {'text': ' what value does this open source project provide to you?', 'timestamp': [32.74, 35.36]}, {'text': " Well, as a developer, traditionally, I'd need to request computing resources", 'timestamp': [35.86, 39.42]}, {'text': ' or hardware to run something as intensive as a large language model.', 'timestamp': [39.42, 43.3]}, {'text': ' And to use cloud services involves sending my data to somebody else, which might not', 'timestamp': [43.7, 48.56]}, {'text': ' always be feasible.', 'timestamp': [48.56, 50]}, {'text': ' So by running models from my local machine, I can maintain full control over my AI and', 'timestamp': [50, 55.62]}, {'text': ' use a model through an API, just like I would with another service like a database on my', 'timestamp': [55.62, 60.12]}, {'text': ' own system.', 'timestamp': [60.12, 61.54]}, {'text': " Let's see this in action by switching over to my laptop and heading to olama.com. And this is where you can install the command line tool for Mac, Windows, and", 'timestamp': [61.54, 69.78]}, {'text': ' of course Linux, but also browse the repository of models. For example, foundation models', 'timestamp': [69.78, 75.16]}, {'text': ' from the leading AI labs, but also more fine-tuned or task-specific models such as code assistants.', 'timestamp': [75.16, 81.72]}, {'text': " Which one should you use? Well, we'll take a look at that soon. But for now, I'll open up my terminal where Ollama has been installed and the first step is downloading and chatting with the model locally.", 'timestamp': [81.72, 91.3]}, {'text': ' So now I have Ollama set up on my local machine.', 'timestamp': [91.52, 94.74]}, {'text': " And what we're gonna do first is use the Ollama run command, which is almost two commands in one.", 'timestamp': [94.74, 99.78]}, {'text': " What's gonna happen is it's going to pull the model from Ollama's model store if we don't already have it,", 'timestamp': [100.12, 105.54]}, {'text': " and also start up an inference server for us to make requests to the LLM that's running on our own machine.", 'timestamp': [105.54, 111.3]}, {'text': " So let's go ahead and do that now.", 'timestamp': [111.3, 112.84]}, {'text': " We're going to run Ollama run granite 3.1 dense.", 'timestamp': [112.84, 116.3]}, {'text': ' And so while we have a chat interface here where we could ask questions,', 'timestamp': [116.3, 120.44]}, {'text': " behind the scenes what we've done is downloaded a quantized or compressed version of a model that's capable of running on limited hardware. And we're also using a backend like", 'timestamp': [120.44, 129.4]}, {'text': ' Lama C++ to run the model. So every time that we ask and chat with the model, for example,', 'timestamp': [129.4, 135.58]}, {'text': " asking Vim or Emacs, what's happening is we're getting our response, but we're also making", 'timestamp': [135.58, 141.04]}, {'text': " a post request to the API that's running on localhost.", 'timestamp': [141.04, 145.08]}, {'text': ' Pretty cool, right? So for our example, I ran the Granite 3.1 model,', 'timestamp': [146.6, 150.02]}, {'text': ' and as a developer, it has a lot of features that are quite interesting to me.', 'timestamp': [150.02, 153.68]}, {'text': ' So it supports 11 different languages', 'timestamp': [153.68, 156.08]}, {'text': ' so it could translate between Spanish and English and back and forth.', 'timestamp': [156.08, 159.12]}, {'text': " And it's also optimized for enterprise specific tasks.", 'timestamp': [159.38, 162.46]}, {'text': ' This includes high benchmarks on rag capabilities,', 'timestamp': [162.86, 165.5]}, {'text': ' which rag allows us to use our unique data with the LLM', 'timestamp': [165.64, 168.64]}, {'text': ' by providing it in the context window of our queries,', 'timestamp': [168.64, 171.24]}, {'text': ' but also capabilities for agentic behavior and much more.', 'timestamp': [171.58, 174.68]}, {'text': " But as always, it's good to keep your options open.", 'timestamp': [174.92, 177.98]}, {'text': ' The O-Lama model catalog is quite impressive with models', 'timestamp': [178.42, 181.46]}, {'text': ' for embedding, vision, tools and many more.', 'timestamp': [181.46, 184.6]}, {'text': ' But you could also import your own fine tuned models for embedding, vision, tools, and many more. But you could', 'timestamp': [184.76, 185.16]}, {'text': ' also import your own fine-tuned models, for example, or use them from Hugging Face by', 'timestamp': [185.16, 190.76]}, {'text': " using what's known as the Ollama model file.", 'timestamp': [190.76, 193.44]}, {'text': " So we've installed Ollama, we've chatted with the model running locally, and we've explored", 'timestamp': [193.44, 197.56]}, {'text': " the model ecosystem. But there's a big question left. What about integrating an LLM like this", 'timestamp': [197.56, 202.6]}, {'text': " into our existing application? So let me hop out of the chat window and let's make sure that the model is", 'timestamp': [202.6, 208.52]}, {'text': ' running locally on our system.', 'timestamp': [208.52, 210.04]}, {'text': ' So Ollama PS can show us the running models.', 'timestamp': [210.08, 213.52]}, {'text': ' And now that we have a model running on localhost,', 'timestamp': [213.56, 216]}, {'text': ' our application needs a way to communicate with this model in a standardized', 'timestamp': [216.2, 219.92]}, {'text': ' format.', 'timestamp': [219.92, 220.76]}, {'text': " That's where we're going to be using what's known as Langchain and specifically", 'timestamp': [220.84, 224.9]}, {'text': " Langchain for Java in our application, which is a framework that's grown in popularity", 'timestamp': [224.9, 229.32]}, {'text': ' and allows us to use a standardized API to make calls to the model from our application', 'timestamp': [229.32, 235.66]}, {'text': " that's written in Java. Now we're going to be using Quarkus, which is a Kubernetes optimized", 'timestamp': [235.66, 241.06]}, {'text': ' Java flavor that supports this Langchainain4J extension in order to call', 'timestamp': [241.06, 246.2]}, {'text': ' our model from the application.', 'timestamp': [246.2, 248.16]}, {'text': " Let's get started.", 'timestamp': [248.16, 249.16]}, {'text': " So, let's take a look at the application that we're currently working on.", 'timestamp': [249.16, 252.52]}, {'text': " So I'll open it up here in the browser.", 'timestamp': [252.52, 254.86]}, {'text': " Now what's happening is that this fictitious organization Parasol is being overwhelmed", 'timestamp': [254.86, 260]}, {'text': ' by new insurance claims and could use the help of an AI like a large language model', 'timestamp': [260, 266.12]}, {'text': ' to help process this overwhelming amount of information and make better and quicker decisions.', 'timestamp': [266.12, 271.92]}, {'text': " But how do we do that behind the scenes? So here in our project, we've added langchain", 'timestamp': [271.92, 275.8]}, {'text': " for j as a dependency and we're going to specify the URL as localhost on port 11434 in our", 'timestamp': [275.8, 282.84]}, {'text': ' application.properties pointing to where our model is running on our machine.', 'timestamp': [282.84, 287.6]}, {'text': " Now, we're also gonna be using a web socket", 'timestamp': [287.6, 289.64]}, {'text': ' in order to make a post request to the model,', 'timestamp': [289.64, 292.24]}, {'text': ' and now our agents have AI capabilities,', 'timestamp': [292.24, 295]}, {'text': ' specifically a helpful assistant that can work with them', 'timestamp': [295, 298.06]}, {'text': ' to complete their job tasks.', 'timestamp': [298.06, 300]}]
- text:  Hey, quick question. Did you know that you can run the latest large language models locally on your...